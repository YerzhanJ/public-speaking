{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/home/mario/pydata2017/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip')\n",
    "sys.path.append('/home/mario/pydata2017/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip')\n",
    "os.environ['SPARK_HOME'] = '/home/mario/pydata2017/spark-2.2.0-bin-hadoop2.7'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 pyspark-shell'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.6'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "\n",
    "json_decoder = lambda s: json.loads(s.decode('ascii'))\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading points db (csv with road conditions and nearest weather station code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_db = spark.read.csv('road_points.csv', header=True)\n",
    "points_dict = spark.sparkContext.broadcast(points_db.rdd.map(lambda r: (r['point_id'], r)).collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming context initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_configuration = {'metadata.broker.list': 'localhost:9092'}\n",
    "car_events_topic = 'car'\n",
    "weather_topic = 'weather'\n",
    "checkpoint_dir = '/tmp/pydata2017-spark'\n",
    "short_window = 1 # minutes\n",
    "long_window = 10 # minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(spark.sparkContext, short_window * 60)\n",
    "ssc.checkpoint(checkpoint_dir)\n",
    "inductive_loop_events = KafkaUtils.createDirectStream(ssc, [car_events_topic], client_configuration, valueDecoder=json_decoder)\n",
    "weather_information = KafkaUtils.createDirectStream(ssc, [weather_topic], client_configuration, valueDecoder=json_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowing (1-minute and 10-minutes) and calculating:\n",
    " - number of cars\n",
    " - average speed\n",
    " - minimium cars gap (in meters)\n",
    " - average gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_record = lambda r: (r[0], (1, r[1]['speed'], r[1]['gap_meters'], r[1]['gap_meters']))\n",
    "get_stats = lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2], __builtin__.min(a[3], b[3]))\n",
    "map_stats = lambda r, mode: (r[0], {\n",
    "    '%s_avg_speed' % mode: r[1][1] / r[1][0],\n",
    "    '%s_avg_gap' % mode: r[1][2] / r[1][0],\n",
    "    '%s_min_gap' % mode: r[1][3],\n",
    "    '%s_num_cars' % mode: r[1][0]\n",
    "})\n",
    "\n",
    "get_window = lambda length: inductive_loop_events.window(length * 60, 60).map(map_record).reduceByKey(get_stats) \\\n",
    "    .map(lambda r: map_stats(r, str(length)))\n",
    "\n",
    "cars_stats = get_window(long_window).join(get_window(short_window)).map(lambda r: (r[0], {**r[1][0], **r[1][1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining loops stats with points db (off-line, previously loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_with_points_data(point):\n",
    "    point[1]['point_data'] = points_dict.value[point[0]]\n",
    "    return (point[1]['point_data'].nearest_weather_station, point[1])\n",
    "\n",
    "cars_stats_with_point_data = cars_stats.map(map_with_points_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating weather stateful RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_function(new_values, last_state):\n",
    "    if len(new_values) == 0: # no weather info in this run\n",
    "        return last_state\n",
    "    return new_values[0]\n",
    "\n",
    "weather_state = weather_information.updateStateByKey(update_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining current point features with weather and mapping to final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_data(point):\n",
    "    return {\n",
    "        'id': point[1][0]['point_data'].point_id,\n",
    "        'road_shape': point[1][0]['point_data'].road_shape,\n",
    "        'allowed_speed': point[1][0]['point_data'].allowed_speed,\n",
    "        'one_minute_avg_speed': point[1][0]['1_avg_speed'],\n",
    "        'one_minute_num_cars': point[1][0]['1_num_cars'],\n",
    "        'one_minute_min_gap': point[1][0]['1_min_gap'],\n",
    "        'one_minute_avg_gap': point[1][0]['1_avg_gap'],\n",
    "        'ten_minutes_avg_speed': point[1][0]['10_avg_speed'],\n",
    "        'ten_minutes_num_cars': point[1][0]['10_num_cars'],\n",
    "        'ten_minutes_min_gap': point[1][0]['10_min_gap'],\n",
    "        'ten_minutes_avg_gap': point[1][0]['10_avg_gap'],\n",
    "        'temperature': point[1][1]['temp'],\n",
    "        'snow': point[1][1]['snow'],\n",
    "        'rain': point[1][1]['rain'],\n",
    "        'ts': point[1][1]['timestamp']\n",
    "    }\n",
    "\n",
    "current_point_status = cars_stats_with_point_data.join(weather_state).map(extract_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(data):\n",
    "    import random\n",
    "    return (data['id'], data['ten_minutes_num_cars']) # here goes ML model\n",
    "current_point_status.map(apply_model).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssc.stop(stopSparkContext=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
